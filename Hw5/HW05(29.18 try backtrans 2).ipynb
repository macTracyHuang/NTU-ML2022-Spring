{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFEKWoh3p1Mv"
      },
      "source": [
        "# Homework Description\n",
        "- English to Chinese (Traditional) Translation\n",
        "  - Input: an English sentence         (e.g.\t\ttom is a student .)\n",
        "  - Output: the Chinese translation  (e.g. \t\t湯姆 是 個 學生 。)\n",
        "\n",
        "- TODO\n",
        "    - Train a simple RNN seq2seq to acheive translation\n",
        "    - Switch to transformer model to boost performance\n",
        "    - Apply Back-translation to furthur boost performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Vf1Q79XPQ3D"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59neB_Sxp5Ub"
      },
      "source": [
        "# Download and import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rRlFbfFRpZYT"
      },
      "outputs": [],
      "source": [
        "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "# !pip install --upgrade jupyter ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fSksMTdmp-Wt"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# !cd fairseq && git checkout 9a1c497\n",
        "# !pip install --upgrade ./fairseq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uRLTiuIuqGNc"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n07Za1XqJzA"
      },
      "source": [
        "# Fix random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xllxxyWxqI7s"
      },
      "outputs": [],
      "source": [
        "seed = 666\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  \n",
        "np.random.seed(seed)  \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ORDJ-2qdYw"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "## En-Zh Bilingual Parallel Corpus\n",
        "* [TED2020](#reimers-2020-multilingual-sentence-bert)\n",
        "    - Raw: 398,066 (sentences)   \n",
        "    - Processed: 393,980 (sentences)\n",
        "    \n",
        "\n",
        "## Testdata\n",
        "- Size: 4,000 (sentences)\n",
        "- **Chinese translation is undisclosed. The provided (.zh) file is psuedo translation, each line is a '。'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQw2mY4Dqkzd"
      },
      "source": [
        "## Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SXT42xQtqijD"
      },
      "outputs": [],
      "source": [
        "data_dir = './DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "# urls = (\n",
        "#     \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted2020.tgz\",\n",
        "#     \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/test.tgz\",\n",
        "# )\n",
        "# file_names = (\n",
        "#     'ted2020.tgz', # train & dev\n",
        "#     'test.tgz', # test\n",
        "# )\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "prefix.mkdir(parents=True, exist_ok=True)\n",
        "# for u, f in zip(urls, file_names):\n",
        "#     path = prefix/f\n",
        "#     if not path.exists():\n",
        "#         !wget {u} -O {path}\n",
        "#     if path.suffix == \".tgz\":\n",
        "#         !tar -xvf {path} -C {prefix}\n",
        "#     elif path.suffix == \".zip\":\n",
        "#         !unzip -o {path} -d {prefix}\n",
        "# !mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "# !mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "# !mv {prefix/'test/test.en'} {prefix/'test.raw.en'}\n",
        "# !mv {prefix/'test/test.zh'} {prefix/'test.raw.zh'}\n",
        "# !rm -rf {prefix/'test'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLkJwNiFrIwZ"
      },
      "source": [
        "## Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_uJYkCncrKJb"
      },
      "outputs": [],
      "source": [
        "# train a zh to en model for back translation\n",
        "src_lang = 'zh'\n",
        "tgt_lang = 'en'\n",
        "\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0t2CPt1brOT3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸。我非常感激。\n",
            "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
            "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
            "請你們設身處地為我想一想！\n",
            "Thank you so much, Chris.\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "And I say that sincerely, partly because  I need that.\n",
            "Put yourselves in my position.\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.'+src_lang} -n 5\n",
        "!head {data_prefix+'.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRoE9UK7r1gY"
      },
      "source": [
        "## Preprocess files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3tzFwtnFrle3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def strQ2B(ustring):\n",
        "    \"\"\"Full width -> half width\"\"\"\n",
        "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # Full width space: direct conversion\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "                \n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace('-', '') # remove '-'\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "    elif lang == 'zh':\n",
        "        s = strQ2B(s) # Q2B\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h_i8b1PRr9Nf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/ted2020/train_dev.raw.clean.zh & en exists. skipping clean.\n",
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/ted2020/test.raw.clean.zh & en exists. skipping clean.\n"
          ]
        }
      ],
      "source": [
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gjT3XCy9r_rj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n",
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
        "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKb4u67-sT_Z"
      },
      "source": [
        "## Split into train/valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AuFKeDz3sGHL"
      },
      "outputs": [],
      "source": [
        "valid_ratio = 0.01 # 3000~4000 would suffice\n",
        "train_ratio = 1 - valid_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QR2NVldqsXyY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train/valid splits exists. skipping split.\n"
          ]
        }
      ],
      "source": [
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1rwQysTsdJq"
      },
      "source": [
        "## Subword Units \n",
        "Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.\n",
        "- We will use the [sentencepiece](#kudo-richardson-2018-sentencepiece) package\n",
        "- select 'unigram' or 'byte-pair encoding (BPE)' algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ecwllsa7sZRA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/ted2020/spm8000.model exists. skipping spm_train.\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' works as well\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lQPRNldqse_V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/ted2020/train.zh exists. skipping spm_encode.\n",
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/ted2020/train.en exists. skipping spm_encode.\n",
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/ted2020/valid.zh exists. skipping spm_encode.\n",
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/ted2020/valid.en exists. skipping spm_encode.\n",
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/ted2020/test.zh exists. skipping spm_encode.\n",
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/ted2020/test.en exists. skipping spm_encode.\n"
          ]
        }
      ],
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4j6lXHjAsjXa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
            "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
            "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對我 之前 演講 的 好 評 ▁。\n",
            "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁!\n",
            "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁!\n",
            "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
            "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁ bl ow n ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
            "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n"
          ]
        }
      ],
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59si_C0Wsms7"
      },
      "source": [
        "## Binarize the data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w-cHVLSpsknh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA/data-bin/ted2020 exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref {prefix/'train'}\\\n",
        "        --validpref {prefix/'valid'}\\\n",
        "        --testpref {prefix/'test'}\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMuH1SWLPWA"
      },
      "source": [
        "# Configuration for experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5Luz3_tVLUxs"
      },
      "outputs": [],
      "source": [
        "# config a zh to en model for back translation\n",
        "\n",
        "config = Namespace(\n",
        "    datadir = \"./DATA/data-bin/ted2020\",\n",
        "    savedir = \"./checkpoints/zhToen\",\n",
        "    source_lang = \"zh\",\n",
        "    target_lang = \"en\",\n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=8192,\n",
        "    accum_steps=2,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=1.5,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=30,\n",
        "    start_epoch=1,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10, \n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjrJFvyQLg86"
      },
      "source": [
        "# Logging\n",
        "- logging package logs ordinary messages\n",
        "- wandb logs the loss, bleu, etc. in the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-ZiMyDWALbDk"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNoSkK45Lmqc"
      },
      "source": [
        "# CUDA Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oqrsbmcoLqMl"
      },
      "outputs": [],
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJuBIHLLt2D"
      },
      "source": [
        "# Dataloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpG4EBRLwe_"
      },
      "source": [
        "## We borrow the TranslationTask from fairseq\n",
        "* used to load the binarized data created above\n",
        "* well-implemented data iterator (dataloader)\n",
        "* built-in task.source_dictionary and task.target_dictionary are also handy\n",
        "* well-implemented beach search decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3gSEy1uFLvVs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 16:49:33 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n",
            "2022-03-20 16:49:33 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n"
          ]
        }
      ],
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mR7Bhov7L4IU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 16:49:33 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2022-03-20 16:49:33 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.zh\n",
            "2022-03-20 16:49:33 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.en\n",
            "2022-03-20 16:49:33 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train zh-en 390041 examples\n",
            "2022-03-20 16:49:33 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.zh\n",
            "2022-03-20 16:49:33 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.en\n",
            "2022-03-20 16:49:33 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid zh-en 3939 examples\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P0BCEm_9L6ig"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([ 140,  690,   28,  270,   45,  151, 1142,  660,  606,  369, 3114, 2434,\n",
            "        1434,  192,    2]),\n",
            " 'target': tensor([  18,   14,    6, 2234,   60,   19,   80,    5,  256,   16,  405, 1407,\n",
            "        1706,    7,    2])}\n",
            "'Source: 這實在就是我所做的--光學操控思想'\n",
            "\"Target: that's exactly what i do optical mind control .\"\n"
          ]
        }
      ],
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcfCVa2FMBSE"
      },
      "source": [
        "# Dataset iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBvc-B_6MKZM"
      },
      "source": [
        "* Controls every batch to contain no more than N tokens, which optimizes GPU memory efficiency\n",
        "* Shuffles the training set for every epoch\n",
        "* Ignore sentences exceeding maximum length\n",
        "* Pad all sentences in a batch to the same length, which enables parallel computing by GPU\n",
        "* Add eos and shift one token\n",
        "    - teacher forcing: to train the model to predict the next token based on prefix, we feed the right shifted target sequence as the decoder input.\n",
        "    - generally, prepending bos to the target would do the job (as shown below)\n",
        "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
        "    - in fairseq however, this is done by moving the eos token to the begining. Empirically, this has the same effect. For instance:\n",
        "    ```\n",
        "    # output target (target) and Decoder input (prev_output_tokens): \n",
        "                   eos = 2\n",
        "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
        "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OWFJFmCnMDXW"
      },
      "outputs": [],
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
        "        # first call of this method has no effect. \n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "# demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "# demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "# sample = next(demo_iter)\n",
        "# sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p86K-0g7Me4M"
      },
      "source": [
        "* each batch is a python dict, with string key and Tensor value. Contents are described below:\n",
        "```python\n",
        "batch = {\n",
        "    \"id\": id, # id for each example \n",
        "    \"nsentences\": len(samples), # batch size (sentences)\n",
        "    \"ntokens\": ntokens, # batch size (tokens)\n",
        "    \"net_input\": {\n",
        "        \"src_tokens\": src_tokens, # sequence in source language\n",
        "        \"src_lengths\": src_lengths, # sequence length of each example before padding\n",
        "        \"prev_output_tokens\": prev_output_tokens, # right shifted target, as mentioned above.\n",
        "    },\n",
        "    \"target\": target, # target sequence\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EyDBE5ZMkFZ"
      },
      "source": [
        "# Model Architecture\n",
        "* We again inherit fairseq's encoder, decoder and model, so that in the testing phase we can directly leverage fairseq's beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Hzh74qLIMfW_"
      },
      "outputs": [],
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder, \n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDAPmxjRNEEL"
      },
      "source": [
        "## Seq2Seq\n",
        "- Composed of **Encoder** and **Decoder**\n",
        "- Recieves inputs and pass to **Encoder** \n",
        "- Pass the outputs from **Encoder** to **Decoder**\n",
        "- **Decoder** will decode according to outputs of previous timesteps as well as **Encoder** outputs  \n",
        "- Once done decoding, return the **Decoder** outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oRwKdLa0NEU6"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens,\n",
        "        src_lengths,\n",
        "        prev_output_tokens,\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens,\n",
        "            encoder_out=encoder_out,\n",
        "            src_lengths=src_lengths,\n",
        "            return_all_hiddens=return_all_hiddens,\n",
        "        )\n",
        "        return logits, extra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu3C2JfqNHzk"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nyI9FOx-NJ2m"
      },
      "outputs": [],
      "source": [
        "# # HINT: transformer architecture\n",
        "from fairseq.models.transformer import (\n",
        "    TransformerEncoder, \n",
        "    TransformerDecoder,\n",
        ")\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # token embeddings\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "    \n",
        "    # encoder decoder\n",
        "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
        "    # encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    # decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "    # sequence to sequence model\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "    \n",
        "    # initialization for seq2seq model is important, requires extra handling\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            # module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=1024 ** -0.5)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        if isinstance(module, MultiheadAttention):\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1)\n",
        "            \n",
        "    # weight initialization\n",
        "    model.apply(init_params)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5n4eS7NQNy"
      },
      "source": [
        "## Architecture Related Configuration\n",
        "\n",
        "For strong baseline, please refer to the hyperparameters for *transformer-base* in Table 3 in [Attention is all you need](#vaswani2017)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cyn30VoGNT6N"
      },
      "outputs": [],
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=512,\n",
        "    encoder_ffn_embed_dim=2048,\n",
        "    encoder_layers=6,\n",
        "    decoder_embed_dim=512,\n",
        "    decoder_ffn_embed_dim=2048,\n",
        "    decoder_layers=6,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.3,\n",
        ")\n",
        "\n",
        "# HINT: these patches on parameters for Transformer\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=4\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=4\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Nbb76QLCNZZZ"
      },
      "outputs": [],
      "source": [
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(arch_args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7ZWfxsCDNatH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 16:49:34 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=512, out_features=8000, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHll7GRNNdqc"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUB9f1WCNgMH"
      },
      "source": [
        "## Loss: Label Smoothing Regularization\n",
        "* let the model learn to generate less concentrated distribution, and prevent over-confidence\n",
        "* sometimes the ground truth may not be the only answer. thus, when calculating loss, we reserve some probability for incorrect labels\n",
        "* avoids overfitting\n",
        "\n",
        "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IgspdJn0NdYF"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduce = reduce\n",
        "    \n",
        "    def forward(self, lprobs, target):\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
        "        # equivalent to summing the log probs of all labels\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index)\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1)\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce:\n",
        "            nll_loss = nll_loss.sum()\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "        # when calculating cross-entropy, add the loss of other labels\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "# generally, 0.1 is good enough\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRalDto2NkJJ"
      },
      "source": [
        "## Optimizer: Adam + lr scheduling\n",
        "Inverse square root scheduling is important to the stability when training Transformer. It's later used on RNN as well.\n",
        "Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.\n",
        "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sS7tQj1ROBYm"
      },
      "outputs": [],
      "source": [
        "def get_rate(d_model, step_num, warmup_step):\n",
        "    # TODO: Change lr from constant to the equation shown above\n",
        "    # lr = 0.001\n",
        "    lr = d_model**(-0.5) * min(step_num**(-0.5), step_num*warmup_step**(-1.5))\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J8hoAjHPNkh3"
      },
      "outputs": [],
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "        \n",
        "    def multiply_grads(self, c):\n",
        "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c)\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFJlkOMONsc6"
      },
      "source": [
        "## Scheduling Visualized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "A135fwPCNrQs"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOR0g-cVO5ZO"
      },
      "source": [
        "# Training Procedure  (backward model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0ZjbK3O8Iv"
      },
      "source": [
        "## Training(backward model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "foal3xM1O404"
      },
      "outputs": [],
      "source": [
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
        "    \n",
        "    stats = {\"loss\": []}\n",
        "    scaler = GradScaler() # automatic mixed precision (amp) \n",
        "    \n",
        "    model.train()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
        "    for samples in progress:\n",
        "        model.zero_grad()\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "        # gradient accumulation: update every accum_steps samples\n",
        "        for i, sample in enumerate(samples):\n",
        "            if i == 1:\n",
        "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size_i = sample[\"ntokens\"]\n",
        "            sample_size += sample_size_i\n",
        "            \n",
        "            # mixed precision training\n",
        "            with autocast():\n",
        "                net_output = model.forward(**sample[\"net_input\"])\n",
        "                lprobs = F.log_softmax(net_output[0], -1)            \n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "                \n",
        "                # logging\n",
        "                accum_loss += loss.item()\n",
        "                # back-prop\n",
        "                scaler.scale(loss).backward()                \n",
        "        \n",
        "        scaler.unscale_(optimizer)\n",
        "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
        "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # logging\n",
        "        loss_print = accum_loss/sample_size\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        progress.set_postfix(loss=loss_print)\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss_print,\n",
        "                \"train/grad_norm\": gnorm.item(),\n",
        "                \"train/lr\": optimizer.rate(),\n",
        "                \"train/sample_size\": sample_size,\n",
        "            })\n",
        "        \n",
        "    loss_print = np.mean(stats[\"loss\"])\n",
        "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1lX3DRO_yU"
      },
      "source": [
        "## Validation & Inference\n",
        "To prevent overfitting, validation is required every epoch to validate the performance on unseen data.\n",
        "- the procedure is essensially same as training, with the addition of inference step\n",
        "- after validation we can save the model weights\n",
        "\n",
        "Validation loss alone cannot describe the actual performance of the model\n",
        "- Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation\n",
        "- We can also manually examine the hypotheses' quality\n",
        "- We use fairseq's sequence generator for beam search to generate translation hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2og80HYQPAKq"
      },
      "outputs": [],
      "source": [
        "# fairseq's beam search generator\n",
        "# given model and input seqeunce, produce translation hypotheses by beam search\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # convert from Tensor to human readable sentence\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y1o7LeDkPDsd"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import sacrebleu\n",
        "\n",
        "def validate(model, task, criterion, log_to_wandb=True):\n",
        "    logger.info('begin validation')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    \n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "            \n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "            \n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "    \n",
        "    if config.use_wandb and log_to_wandb:\n",
        "        wandb.log({\n",
        "            \"valid/loss\": stats[\"loss\"],\n",
        "            \"valid/bleu\": stats[\"bleu\"].score,\n",
        "        }, commit=False)\n",
        "    \n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source: \" + srcs[showid])\n",
        "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
        "    logger.info(\"example reference: \" + refs[showid])\n",
        "    \n",
        "    # show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRF6nd4PGEE"
      },
      "source": [
        "# Save and Load Model Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "edBuLlkuPGr9"
      },
      "outputs": [],
      "source": [
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
        "    stats = validate(model, task, criterion)\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        # save epoch checkpoints\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "    \n",
        "        # save epoch samples\n",
        "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        # get best valid bleu    \n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "            \n",
        "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "        if del_file.exists():\n",
        "            del_file.unlink()\n",
        "    \n",
        "    return stats\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir)/name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath)\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyIFpibfPJ5u"
      },
      "source": [
        "# Main\n",
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hu7RZbCUPKQr"
      },
      "outputs": [],
      "source": [
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5xxlJxU2PeAo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 16:49:38 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2022-03-20 16:49:38 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2022-03-20 16:49:38 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2022-03-20 16:49:38 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-03-20 16:49:38 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2022-03-20 16:49:38 | INFO | hw5.seq2seq | num. model params: 52,332,544 (num. trained: 52,332,544)\n",
            "2022-03-20 16:49:38 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MSPRqpQUPfaX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 16:49:38 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326674]\n",
            "2022-03-20 16:49:38 | INFO | hw5.seq2seq | loaded checkpoint checkpoints/zhToen/checkpoint_last.pt: step=3940 loss=3.0035789012908936 bleu=14.278902788913516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 16:53:23 | INFO | hw5.seq2seq | training loss: 3.0710\n",
            "2022-03-20 16:53:23 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "validation:   0%|          | 0/26 [00:00<?, ?it/s, valid_loss=2.58]/home/tracy/miniconda3/envs/torch/lib/python3.9/site-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/home/tracy/miniconda3/envs/torch/lib/python3.9/site-packages/fairseq/sequence_generator.py:657: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n",
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 16:53:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 16:53:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 16:53:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 16:53:45 | INFO | hw5.seq2seq | example source: 它是個美麗的國家 , 位在西非 。\n",
            "2022-03-20 16:53:45 | INFO | hw5.seq2seq | example hypothesis: it's a beautiful country in west africa .\n",
            "2022-03-20 16:53:45 | INFO | hw5.seq2seq | example reference: it is a beautiful country located in west africa .\n",
            "2022-03-20 16:53:45 | INFO | hw5.seq2seq | validation loss:\t2.9056\n",
            "2022-03-20 16:53:45 | INFO | hw5.seq2seq | BLEU = 15.85 53.6/24.1/12.5/6.7 (BP = 0.874 ratio = 0.882 hyp_len = 67933 ref_len = 77050)\n",
            "2022-03-20 16:53:46 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint1.pt\n",
            "2022-03-20 16:53:46 | INFO | hw5.seq2seq | end of epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 16:57:29 | INFO | hw5.seq2seq | training loss: 2.9428\n",
            "2022-03-20 16:57:29 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 16:57:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 16:57:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 16:57:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 16:57:50 | INFO | hw5.seq2seq | example source: 他懂得策略 。\n",
            "2022-03-20 16:57:50 | INFO | hw5.seq2seq | example hypothesis: he understood strategy .\n",
            "2022-03-20 16:57:50 | INFO | hw5.seq2seq | example reference: arthur samuel knew strategy .\n",
            "2022-03-20 16:57:50 | INFO | hw5.seq2seq | validation loss:\t2.8282\n",
            "2022-03-20 16:57:50 | INFO | hw5.seq2seq | BLEU = 15.89 55.5/25.5/13.4/7.2 (BP = 0.826 ratio = 0.840 hyp_len = 64686 ref_len = 77050)\n",
            "2022-03-20 16:57:51 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint2.pt\n",
            "2022-03-20 16:57:51 | INFO | hw5.seq2seq | end of epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:01:35 | INFO | hw5.seq2seq | training loss: 2.8487\n",
            "2022-03-20 17:01:35 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:01:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:01:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:01:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:01:56 | INFO | hw5.seq2seq | example source: 你可以成為傑出的一代 。 」\n",
            "2022-03-20 17:01:56 | INFO | hw5.seq2seq | example hypothesis: you can be a brilliant generation . \"\n",
            "2022-03-20 17:01:56 | INFO | hw5.seq2seq | example reference: you can be that great generation . \"\n",
            "2022-03-20 17:01:56 | INFO | hw5.seq2seq | validation loss:\t2.7817\n",
            "2022-03-20 17:01:56 | INFO | hw5.seq2seq | BLEU = 17.05 56.0/26.4/14.2/7.9 (BP = 0.845 ratio = 0.855 hyp_len = 65913 ref_len = 77050)\n",
            "2022-03-20 17:01:57 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint3.pt\n",
            "2022-03-20 17:01:58 | INFO | hw5.seq2seq | end of epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:05:42 | INFO | hw5.seq2seq | training loss: 2.7714\n",
            "2022-03-20 17:05:42 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:06:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:06:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:06:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:06:03 | INFO | hw5.seq2seq | example source: 它驅使人們離開大腦聰明的那部份那個潛意識的黑暗深井本能和經驗所在的地方以及所有其他創意的元素還有良好判斷力所在之處它迫使我們去到單薄又呆板有意識的邏輯 。\n",
            "2022-03-20 17:06:03 | INFO | hw5.seq2seq | example hypothesis: it drives people out of the smart parts of the unconsciousness of the subconsciousness of the experience , where the elements of all other creativity and good judgments , and it forces us to go to a very thin and silly logic .\n",
            "2022-03-20 17:06:03 | INFO | hw5.seq2seq | example reference: it drives people from the smart part of the brain that dark , deep well of the subconscious , where instincts and experience , and all the other factors of creativity and good judgment are it drives us to the thin veneer of conscious logic .\n",
            "2022-03-20 17:06:03 | INFO | hw5.seq2seq | validation loss:\t2.7285\n",
            "2022-03-20 17:06:03 | INFO | hw5.seq2seq | BLEU = 18.28 54.9/26.0/14.1/7.9 (BP = 0.915 ratio = 0.918 hyp_len = 70767 ref_len = 77050)\n",
            "2022-03-20 17:06:04 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint4.pt\n",
            "2022-03-20 17:06:05 | INFO | hw5.seq2seq | end of epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:09:49 | INFO | hw5.seq2seq | training loss: 2.7513\n",
            "2022-03-20 17:09:49 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:10:11 | INFO | hw5.seq2seq | example source: 「 事情並無好壞/是我們的思維使之如此 」\n",
            "2022-03-20 17:10:11 | INFO | hw5.seq2seq | example hypothesis: \" things aren't bad or bad or are our minds . \"\n",
            "2022-03-20 17:10:11 | INFO | hw5.seq2seq | example reference: that seems like a onequestion iq test .\n",
            "2022-03-20 17:10:11 | INFO | hw5.seq2seq | validation loss:\t2.6988\n",
            "2022-03-20 17:10:11 | INFO | hw5.seq2seq | BLEU = 18.49 55.5/26.6/14.5/8.1 (BP = 0.905 ratio = 0.909 hyp_len = 70056 ref_len = 77050)\n",
            "2022-03-20 17:10:11 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint5.pt\n",
            "2022-03-20 17:10:12 | INFO | hw5.seq2seq | end of epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:13:55 | INFO | hw5.seq2seq | training loss: 2.7050\n",
            "2022-03-20 17:13:55 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:14:17 | INFO | hw5.seq2seq | example source: 謝謝 。\n",
            "2022-03-20 17:14:17 | INFO | hw5.seq2seq | example hypothesis: thank you very much .\n",
            "2022-03-20 17:14:17 | INFO | hw5.seq2seq | example reference: thank you .\n",
            "2022-03-20 17:14:17 | INFO | hw5.seq2seq | validation loss:\t2.6726\n",
            "2022-03-20 17:14:17 | INFO | hw5.seq2seq | BLEU = 18.86 56.0/27.0/15.0/8.6 (BP = 0.899 ratio = 0.904 hyp_len = 69617 ref_len = 77050)\n",
            "2022-03-20 17:14:17 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint6.pt\n",
            "2022-03-20 17:14:18 | INFO | hw5.seq2seq | end of epoch 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:18:02 | INFO | hw5.seq2seq | training loss: 2.6681\n",
            "2022-03-20 17:18:02 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:18:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:18:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:18:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:18:23 | INFO | hw5.seq2seq | example source: 不 。\n",
            "2022-03-20 17:18:23 | INFO | hw5.seq2seq | example hypothesis: no .\n",
            "2022-03-20 17:18:23 | INFO | hw5.seq2seq | example reference: no .\n",
            "2022-03-20 17:18:23 | INFO | hw5.seq2seq | validation loss:\t2.6579\n",
            "2022-03-20 17:18:23 | INFO | hw5.seq2seq | BLEU = 19.15 57.3/28.1/15.7/9.0 (BP = 0.877 ratio = 0.884 hyp_len = 68085 ref_len = 77050)\n",
            "2022-03-20 17:18:24 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint7.pt\n",
            "2022-03-20 17:18:25 | INFO | hw5.seq2seq | end of epoch 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:22:07 | INFO | hw5.seq2seq | training loss: 2.6363\n",
            "2022-03-20 17:22:07 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:22:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:22:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:22:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:22:29 | INFO | hw5.seq2seq | example source: 自行車是只在美國南部一些城市正在進行的革命\n",
            "2022-03-20 17:22:29 | INFO | hw5.seq2seq | example hypothesis: bicycles are revolutions in the southern part of the country .\n",
            "2022-03-20 17:22:29 | INFO | hw5.seq2seq | example reference: bicycles and bicycling are the current revolution underway in only some american cities .\n",
            "2022-03-20 17:22:29 | INFO | hw5.seq2seq | validation loss:\t2.6362\n",
            "2022-03-20 17:22:29 | INFO | hw5.seq2seq | BLEU = 19.67 57.4/28.4/15.9/9.2 (BP = 0.890 ratio = 0.896 hyp_len = 69031 ref_len = 77050)\n",
            "2022-03-20 17:22:30 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint8.pt\n",
            "2022-03-20 17:22:30 | INFO | hw5.seq2seq | end of epoch 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:26:14 | INFO | hw5.seq2seq | training loss: 2.6082\n",
            "2022-03-20 17:26:14 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:26:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:26:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:26:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:26:35 | INFO | hw5.seq2seq | example source: jc:我覺得很有意思 。\n",
            "2022-03-20 17:26:35 | INFO | hw5.seq2seq | example hypothesis: jc: i think it's interesting .\n",
            "2022-03-20 17:26:35 | INFO | hw5.seq2seq | example reference: jc: it makes perfect sense to me .\n",
            "2022-03-20 17:26:35 | INFO | hw5.seq2seq | validation loss:\t2.6288\n",
            "2022-03-20 17:26:35 | INFO | hw5.seq2seq | BLEU = 19.29 57.4/28.2/15.7/9.0 (BP = 0.881 ratio = 0.888 hyp_len = 68398 ref_len = 77050)\n",
            "2022-03-20 17:26:36 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint9.pt\n",
            "2022-03-20 17:26:36 | INFO | hw5.seq2seq | end of epoch 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:30:21 | INFO | hw5.seq2seq | training loss: 2.5839\n",
            "2022-03-20 17:30:21 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:30:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:30:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:30:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:30:43 | INFO | hw5.seq2seq | example source: 這就是問題所在彼得‧德森指出了這點他用的例子是某種鴨嘴恐龍當時被稱為亞冠龍\n",
            "2022-03-20 17:30:43 | INFO | hw5.seq2seq | example hypothesis: and that's the problem , and peter dson pointed out that he uses an example of a duck dinosaur called champion .\n",
            "2022-03-20 17:30:43 | INFO | hw5.seq2seq | example reference: so this was a problem , and peter dodson pointed this out using some duckbilled dinosaurs then called hypacrosaurus .\n",
            "2022-03-20 17:30:43 | INFO | hw5.seq2seq | validation loss:\t2.6222\n",
            "2022-03-20 17:30:43 | INFO | hw5.seq2seq | BLEU = 19.26 57.9/28.5/16.0/9.2 (BP = 0.867 ratio = 0.875 hyp_len = 67428 ref_len = 77050)\n",
            "2022-03-20 17:30:44 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint10.pt\n",
            "2022-03-20 17:30:44 | INFO | hw5.seq2seq | end of epoch 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:34:27 | INFO | hw5.seq2seq | training loss: 2.5629\n",
            "2022-03-20 17:34:27 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:34:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:34:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:34:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:34:49 | INFO | hw5.seq2seq | example source: 她說: 「 放下你的身段 , 錢是『綠色』的 。 」\n",
            "2022-03-20 17:34:49 | INFO | hw5.seq2seq | example hypothesis: she said , \" let's put down your status , money is green . \"\n",
            "2022-03-20 17:34:49 | INFO | hw5.seq2seq | example reference: she said , \" get off your throne . money is green . \"\n",
            "2022-03-20 17:34:49 | INFO | hw5.seq2seq | validation loss:\t2.6043\n",
            "2022-03-20 17:34:49 | INFO | hw5.seq2seq | BLEU = 20.43 56.3/27.9/15.8/9.2 (BP = 0.935 ratio = 0.937 hyp_len = 72188 ref_len = 77050)\n",
            "2022-03-20 17:34:50 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint11.pt\n",
            "2022-03-20 17:34:51 | INFO | hw5.seq2seq | end of epoch 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:38:34 | INFO | hw5.seq2seq | training loss: 2.5429\n",
            "2022-03-20 17:38:34 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:38:56 | INFO | hw5.seq2seq | example source: 成為領導者是我的抱負 , 我想問 , 在沒有影響力時 , 你要如何領導 ?\n",
            "2022-03-20 17:38:56 | INFO | hw5.seq2seq | example hypothesis: being a leader is my aspiration , and i want to ask , how do you lead without influence ?\n",
            "2022-03-20 17:38:56 | INFO | hw5.seq2seq | example reference: i'm an aspiring leader , and i have a question about how you lead when you have no influence .\n",
            "2022-03-20 17:38:56 | INFO | hw5.seq2seq | validation loss:\t2.5945\n",
            "2022-03-20 17:38:56 | INFO | hw5.seq2seq | BLEU = 20.77 56.1/28.0/15.9/9.3 (BP = 0.947 ratio = 0.948 hyp_len = 73044 ref_len = 77050)\n",
            "2022-03-20 17:38:57 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint12.pt\n",
            "2022-03-20 17:38:57 | INFO | hw5.seq2seq | end of epoch 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:42:42 | INFO | hw5.seq2seq | training loss: 2.5248\n",
            "2022-03-20 17:42:42 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:43:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:43:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:43:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:43:04 | INFO | hw5.seq2seq | example source: 這樣 , 英國國教的牧師就可以教訓無神論者 , 說他們的言論有多麼冒犯人 ;\n",
            "2022-03-20 17:43:04 | INFO | hw5.seq2seq | example hypothesis: in doing so , the priests of the british state can teach the atheist how offensive they are .\n",
            "2022-03-20 17:43:04 | INFO | hw5.seq2seq | example reference: so anglican ministers could lecture atheists on the offensiveness of their discourse .\n",
            "2022-03-20 17:43:04 | INFO | hw5.seq2seq | validation loss:\t2.5931\n",
            "2022-03-20 17:43:04 | INFO | hw5.seq2seq | BLEU = 20.21 58.4/29.4/16.7/9.7 (BP = 0.880 ratio = 0.887 hyp_len = 68314 ref_len = 77050)\n",
            "2022-03-20 17:43:05 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint13.pt\n",
            "2022-03-20 17:43:05 | INFO | hw5.seq2seq | end of epoch 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:46:49 | INFO | hw5.seq2seq | training loss: 2.5068\n",
            "2022-03-20 17:46:49 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:47:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:47:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:47:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:47:11 | INFO | hw5.seq2seq | example source: 所以 , 我們說 , 好 , 咱們來研究看看這是如何發生的 , 先看科學 。\n",
            "2022-03-20 17:47:11 | INFO | hw5.seq2seq | example hypothesis: so we said , ok , let's look at how this is happening , first looking at science .\n",
            "2022-03-20 17:47:11 | INFO | hw5.seq2seq | example reference: so we said , ok , let's figure out how does this really happen , first in science .\n",
            "2022-03-20 17:47:11 | INFO | hw5.seq2seq | validation loss:\t2.5821\n",
            "2022-03-20 17:47:11 | INFO | hw5.seq2seq | BLEU = 20.65 57.5/28.9/16.4/9.6 (BP = 0.913 ratio = 0.917 hyp_len = 70626 ref_len = 77050)\n",
            "2022-03-20 17:47:11 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint14.pt\n",
            "2022-03-20 17:47:11 | INFO | hw5.seq2seq | end of epoch 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:50:55 | INFO | hw5.seq2seq | training loss: 2.4922\n",
            "2022-03-20 17:50:55 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:51:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:51:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:51:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:51:17 | INFO | hw5.seq2seq | example source: 然而此時此刻我們滅絕這些病症的能力也已經達到了超乎想像的高峰\n",
            "2022-03-20 17:51:17 | INFO | hw5.seq2seq | example hypothesis: but at this point , our ability to extinct these symptoms has reached peaks that are beyond our imagination .\n",
            "2022-03-20 17:51:17 | INFO | hw5.seq2seq | example reference: and yet we also live at the moment when our ability to eliminate those conditions has reached a height we never imagined before .\n",
            "2022-03-20 17:51:17 | INFO | hw5.seq2seq | validation loss:\t2.5756\n",
            "2022-03-20 17:51:17 | INFO | hw5.seq2seq | BLEU = 21.03 55.9/28.0/15.9/9.2 (BP = 0.961 ratio = 0.962 hyp_len = 74117 ref_len = 77050)\n",
            "2022-03-20 17:51:18 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint15.pt\n",
            "2022-03-20 17:51:19 | INFO | hw5.seq2seq | end of epoch 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:55:04 | INFO | hw5.seq2seq | training loss: 2.4801\n",
            "2022-03-20 17:55:04 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:55:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:55:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:55:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:55:26 | INFO | hw5.seq2seq | example source: 是因為勤奮的工作 。 是因為我們發現了有關恐怖活動的情報並且以不同的方式禁止 , 通過執法 , 通過和其他國家的合作 , 而且有時候通過軍事活動 。\n",
            "2022-03-20 17:55:26 | INFO | hw5.seq2seq | example hypothesis: it's because of diligent work , because we discovered intelligence about terrorism and banned it in a different way , through law enforcement , through collaboration with other countries and sometimes through military activities .\n",
            "2022-03-20 17:55:26 | INFO | hw5.seq2seq | example reference: that's hard work . that's us finding intelligence on terrorist activities and interdicting them through one way or another , through law enforcement , through cooperative activities with other countries and sometimes through military action .\n",
            "2022-03-20 17:55:26 | INFO | hw5.seq2seq | validation loss:\t2.5645\n",
            "2022-03-20 17:55:26 | INFO | hw5.seq2seq | BLEU = 20.86 57.6/29.0/16.6/9.7 (BP = 0.915 ratio = 0.918 hyp_len = 70756 ref_len = 77050)\n",
            "2022-03-20 17:55:26 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint16.pt\n",
            "2022-03-20 17:55:26 | INFO | hw5.seq2seq | end of epoch 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:59:12 | INFO | hw5.seq2seq | training loss: 2.4653\n",
            "2022-03-20 17:59:12 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 17:59:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 17:59:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 17:59:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 17:59:34 | INFO | hw5.seq2seq | example source: 這是過去40年來女性不斷倡導的結果\n",
            "2022-03-20 17:59:34 | INFO | hw5.seq2seq | example hypothesis: this is what women have been advocating for over the last 40 years .\n",
            "2022-03-20 17:59:34 | INFO | hw5.seq2seq | example reference: and that is the 40 years that women have advocated .\n",
            "2022-03-20 17:59:34 | INFO | hw5.seq2seq | validation loss:\t2.5658\n",
            "2022-03-20 17:59:34 | INFO | hw5.seq2seq | BLEU = 20.69 57.8/29.2/16.7/9.9 (BP = 0.902 ratio = 0.906 hyp_len = 69834 ref_len = 77050)\n",
            "2022-03-20 17:59:35 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint17.pt\n",
            "2022-03-20 17:59:35 | INFO | hw5.seq2seq | end of epoch 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:03:18 | INFO | hw5.seq2seq | training loss: 2.4534\n",
            "2022-03-20 18:03:18 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:03:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:03:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:03:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:03:40 | INFO | hw5.seq2seq | example source: 所有人都安全了 。\n",
            "2022-03-20 18:03:40 | INFO | hw5.seq2seq | example hypothesis: everyone's safe .\n",
            "2022-03-20 18:03:40 | INFO | hw5.seq2seq | example reference: and no one got killed .\n",
            "2022-03-20 18:03:40 | INFO | hw5.seq2seq | validation loss:\t2.5615\n",
            "2022-03-20 18:03:40 | INFO | hw5.seq2seq | BLEU = 21.16 58.3/29.7/17.1/10.2 (BP = 0.902 ratio = 0.907 hyp_len = 69849 ref_len = 77050)\n",
            "2022-03-20 18:03:41 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint18.pt\n",
            "2022-03-20 18:03:41 | INFO | hw5.seq2seq | end of epoch 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:07:27 | INFO | hw5.seq2seq | training loss: 2.4411\n",
            "2022-03-20 18:07:27 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:07:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:07:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:07:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:07:49 | INFO | hw5.seq2seq | example source: 其他人則是靠著很不適合的獨木舟 , 孤注一擲試圖前往西班牙 。\n",
            "2022-03-20 18:07:49 | INFO | hw5.seq2seq | example hypothesis: others use very bad canoes to try to travel to spain .\n",
            "2022-03-20 18:07:49 | INFO | hw5.seq2seq | example reference: others end up on inadequate wooden canoes in desperate attempts to reach spain .\n",
            "2022-03-20 18:07:49 | INFO | hw5.seq2seq | validation loss:\t2.5591\n",
            "2022-03-20 18:07:49 | INFO | hw5.seq2seq | BLEU = 21.12 58.1/29.5/17.0/10.1 (BP = 0.906 ratio = 0.910 hyp_len = 70108 ref_len = 77050)\n",
            "2022-03-20 18:07:50 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint19.pt\n",
            "2022-03-20 18:07:50 | INFO | hw5.seq2seq | end of epoch 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:11:35 | INFO | hw5.seq2seq | training loss: 2.4303\n",
            "2022-03-20 18:11:35 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:11:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:11:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:11:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:11:57 | INFO | hw5.seq2seq | example source: 對了 , 如果你還沒有注意到 , 我是黑人 , 謝謝 。 如果你像我一樣 , 生長在種族隔離的城市 , 例如芝加哥 , 你自然而然就相信膚色和種族永遠是分不開的 。\n",
            "2022-03-20 18:11:57 | INFO | hw5.seq2seq | example hypothesis: by the way , if you haven't noticed that i am black , thank you , and if you grow in a segregated city , like in chicago , you naturally believe that color and race are never separated .\n",
            "2022-03-20 18:11:57 | INFO | hw5.seq2seq | example reference: now , if you haven't noticed , i am black , thank you and when you grow up in a segregated city as i have , like chicago , you're conditioned to believe that color and race can never be separate .\n",
            "2022-03-20 18:11:57 | INFO | hw5.seq2seq | validation loss:\t2.5569\n",
            "2022-03-20 18:11:57 | INFO | hw5.seq2seq | BLEU = 21.21 57.7/29.2/16.8/9.9 (BP = 0.922 ratio = 0.925 hyp_len = 71245 ref_len = 77050)\n",
            "2022-03-20 18:11:57 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint20.pt\n",
            "2022-03-20 18:11:58 | INFO | hw5.seq2seq | end of epoch 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:15:41 | INFO | hw5.seq2seq | training loss: 2.4199\n",
            "2022-03-20 18:15:41 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:16:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:16:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:16:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:16:03 | INFO | hw5.seq2seq | example source: 如果你是使用蘋果的keynote , 就有更好的版本能用\n",
            "2022-03-20 18:16:03 | INFO | hw5.seq2seq | example hypothesis: if you're using the kynote of apple , there's a better version of it .\n",
            "2022-03-20 18:16:03 | INFO | hw5.seq2seq | example reference: if you use apple's keynote , it's got an even better version .\n",
            "2022-03-20 18:16:03 | INFO | hw5.seq2seq | validation loss:\t2.5547\n",
            "2022-03-20 18:16:03 | INFO | hw5.seq2seq | BLEU = 21.29 57.6/29.2/16.8/10.0 (BP = 0.923 ratio = 0.926 hyp_len = 71338 ref_len = 77050)\n",
            "2022-03-20 18:16:04 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint21.pt\n",
            "2022-03-20 18:16:05 | INFO | hw5.seq2seq | end of epoch 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:19:50 | INFO | hw5.seq2seq | training loss: 2.4101\n",
            "2022-03-20 18:19:50 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:20:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:20:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:20:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:20:12 | INFO | hw5.seq2seq | example source: 這是1970年中國的收入分配\n",
            "2022-03-20 18:20:12 | INFO | hw5.seq2seq | example hypothesis: this is china's income distribution in 1970 .\n",
            "2022-03-20 18:20:12 | INFO | hw5.seq2seq | example reference: this is the income distribution of china , 1970 .\n",
            "2022-03-20 18:20:12 | INFO | hw5.seq2seq | validation loss:\t2.5543\n",
            "2022-03-20 18:20:12 | INFO | hw5.seq2seq | BLEU = 21.09 58.4/29.6/17.0/10.1 (BP = 0.902 ratio = 0.907 hyp_len = 69856 ref_len = 77050)\n",
            "2022-03-20 18:20:13 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint22.pt\n",
            "2022-03-20 18:20:13 | INFO | hw5.seq2seq | end of epoch 22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:23:58 | INFO | hw5.seq2seq | training loss: 2.4010\n",
            "2022-03-20 18:23:58 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:24:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:24:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:24:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:24:21 | INFO | hw5.seq2seq | example source: 他是律師兼作家 , 笑的時候眼睛閃閃發光 , 我親他的時候 , 他緊緊閉上雙眼 , 在那晚的某一刻 , 我們的第零次約會\n",
            "2022-03-20 18:24:21 | INFO | hw5.seq2seq | example hypothesis: he's a lawyer and a writer , and he's laughing , and he's flashing , and when i first saw him , he closes his eyes , and at some point , our zero date that night .\n",
            "2022-03-20 18:24:21 | INFO | hw5.seq2seq | example reference: he was a lawyer and a writer , and his eyes twinkled when he laughed and they squeezed tight when i kissed him and at some point in the evening , our zero date became a first date .\n",
            "2022-03-20 18:24:21 | INFO | hw5.seq2seq | validation loss:\t2.5478\n",
            "2022-03-20 18:24:21 | INFO | hw5.seq2seq | BLEU = 21.27 57.8/29.3/16.9/10.1 (BP = 0.917 ratio = 0.920 hyp_len = 70888 ref_len = 77050)\n",
            "2022-03-20 18:24:21 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint23.pt\n",
            "2022-03-20 18:24:21 | INFO | hw5.seq2seq | end of epoch 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:28:05 | INFO | hw5.seq2seq | training loss: 2.3919\n",
            "2022-03-20 18:28:05 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:28:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:28:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:28:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:28:27 | INFO | hw5.seq2seq | example source: 用特斯拉 , 因你要有永續能源 , 所以造出這麽勁爆的車子來實現你的目標 。\n",
            "2022-03-20 18:28:27 | INFO | hw5.seq2seq | example hypothesis: using tesla , because you've got to have sustainable energy , you've got to create this explosive car to realize your goals .\n",
            "2022-03-20 18:28:27 | INFO | hw5.seq2seq | example reference: with tesla , you want to have sustainable energy , so you made these super sexy , exciting cars to do it .\n",
            "2022-03-20 18:28:27 | INFO | hw5.seq2seq | validation loss:\t2.5506\n",
            "2022-03-20 18:28:27 | INFO | hw5.seq2seq | BLEU = 21.41 57.9/29.4/17.0/10.1 (BP = 0.922 ratio = 0.925 hyp_len = 71286 ref_len = 77050)\n",
            "2022-03-20 18:28:28 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint24.pt\n",
            "2022-03-20 18:28:29 | INFO | hw5.seq2seq | end of epoch 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:32:16 | INFO | hw5.seq2seq | training loss: 2.3847\n",
            "2022-03-20 18:32:16 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:32:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:32:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:32:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:32:38 | INFO | hw5.seq2seq | example source: 你要做的 , 就是把你的問題和以前別人所遇到的問題做比對 , 再利用他們已經想出的辦法來解決 。\n",
            "2022-03-20 18:32:38 | INFO | hw5.seq2seq | example hypothesis: all you have to do is compare your problems to the problems that other people have faced before , and then use the solutions that they've figured out .\n",
            "2022-03-20 18:32:38 | INFO | hw5.seq2seq | example reference: because what you can do is take your problem , and turn it into a problem that someone else has solved , and use their solutions .\n",
            "2022-03-20 18:32:38 | INFO | hw5.seq2seq | validation loss:\t2.5423\n",
            "2022-03-20 18:32:38 | INFO | hw5.seq2seq | BLEU = 21.54 57.6/29.3/16.9/10.0 (BP = 0.933 ratio = 0.935 hyp_len = 72027 ref_len = 77050)\n",
            "2022-03-20 18:32:39 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint25.pt\n",
            "2022-03-20 18:32:40 | INFO | hw5.seq2seq | end of epoch 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:36:24 | INFO | hw5.seq2seq | training loss: 2.3775\n",
            "2022-03-20 18:36:24 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:36:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:36:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:36:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:36:45 | INFO | hw5.seq2seq | example source: 在同樣的狀況下他們該怎麼做 ?\n",
            "2022-03-20 18:36:45 | INFO | hw5.seq2seq | example hypothesis: what do they do in the same situation ?\n",
            "2022-03-20 18:36:45 | INFO | hw5.seq2seq | example reference: what would they do under the same conditions ?\n",
            "2022-03-20 18:36:45 | INFO | hw5.seq2seq | validation loss:\t2.5467\n",
            "2022-03-20 18:36:45 | INFO | hw5.seq2seq | BLEU = 21.55 58.3/29.9/17.3/10.3 (BP = 0.913 ratio = 0.917 hyp_len = 70650 ref_len = 77050)\n",
            "2022-03-20 18:36:46 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint26.pt\n",
            "2022-03-20 18:36:47 | INFO | hw5.seq2seq | end of epoch 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:40:30 | INFO | hw5.seq2seq | training loss: 2.3686\n",
            "2022-03-20 18:40:30 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:40:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:40:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:40:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:40:52 | INFO | hw5.seq2seq | example source: 這帶給我想到第三個假設而這也許是問題最大的假設: \" 永遠不要對選擇說不 \" 。\n",
            "2022-03-20 18:40:52 | INFO | hw5.seq2seq | example hypothesis: and that brings me to my third assumption , and maybe that's the biggest assumption: \" never say no to the choice . \"\n",
            "2022-03-20 18:40:52 | INFO | hw5.seq2seq | example reference: this brings me to the third , and perhaps most problematic , assumption: \" you must never say no to choice . \"\n",
            "2022-03-20 18:40:52 | INFO | hw5.seq2seq | validation loss:\t2.5405\n",
            "2022-03-20 18:40:52 | INFO | hw5.seq2seq | BLEU = 21.37 58.3/29.8/17.2/10.2 (BP = 0.911 ratio = 0.915 hyp_len = 70486 ref_len = 77050)\n",
            "2022-03-20 18:40:53 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint27.pt\n",
            "2022-03-20 18:40:53 | INFO | hw5.seq2seq | end of epoch 27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:44:37 | INFO | hw5.seq2seq | training loss: 2.3612\n",
            "2022-03-20 18:44:37 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:44:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:44:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:44:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:44:59 | INFO | hw5.seq2seq | example source: 沒有錢乘車 , 他們通常都會坐在卡車頂上 , 在這裡則是坐在橫越南蘇丹的火車頂上 。\n",
            "2022-03-20 18:44:59 | INFO | hw5.seq2seq | example hypothesis: without money , they usually sit on the roof of a truck , and here they are on the roof of a train across south sudan .\n",
            "2022-03-20 18:44:59 | INFO | hw5.seq2seq | example reference: with no money for rides , they often made the mzungu ride on the roof of the trucks , or in this case , on the top of the train going across south sudan .\n",
            "2022-03-20 18:44:59 | INFO | hw5.seq2seq | validation loss:\t2.5388\n",
            "2022-03-20 18:44:59 | INFO | hw5.seq2seq | BLEU = 21.58 58.3/29.8/17.3/10.3 (BP = 0.916 ratio = 0.919 hyp_len = 70818 ref_len = 77050)\n",
            "2022-03-20 18:45:00 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint28.pt\n",
            "2022-03-20 18:45:01 | INFO | hw5.seq2seq | end of epoch 28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:48:48 | INFO | hw5.seq2seq | training loss: 2.3543\n",
            "2022-03-20 18:48:48 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:49:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:49:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:49:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:49:09 | INFO | hw5.seq2seq | example source: 我可以把鏡頭向右旋轉一點 , 你就會失焦 , 而背景的人會凸顯出來 。\n",
            "2022-03-20 18:49:09 | INFO | hw5.seq2seq | example hypothesis: i can rotate the camera a little bit to the right , and you lose the focus , and the background reveals it .\n",
            "2022-03-20 18:49:09 | INFO | hw5.seq2seq | example reference: i could move the lens a little to the right , and you would go back and the folks in the background would come out .\n",
            "2022-03-20 18:49:09 | INFO | hw5.seq2seq | validation loss:\t2.5400\n",
            "2022-03-20 18:49:09 | INFO | hw5.seq2seq | BLEU = 21.28 58.4/29.9/17.3/10.3 (BP = 0.902 ratio = 0.906 hyp_len = 69827 ref_len = 77050)\n",
            "2022-03-20 18:49:10 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint29.pt\n",
            "2022-03-20 18:49:10 | INFO | hw5.seq2seq | end of epoch 29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:52:54 | INFO | hw5.seq2seq | training loss: 2.3467\n",
            "2022-03-20 18:52:54 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 18:53:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 18:53:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 18:53:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 18:53:17 | INFO | hw5.seq2seq | example source: 身為白人女性 , 我們得要做更多 , 因為種族主義、性別主義 , 和恐同性戀症 , 是影響我們所有人的議題 。\n",
            "2022-03-20 18:53:17 | INFO | hw5.seq2seq | example hypothesis: as white women , we need to do more because racism , gender and terrorism are issues that affect all of us .\n",
            "2022-03-20 18:53:17 | INFO | hw5.seq2seq | example reference: and as white women , we have to do more , because racism and sexism and homophobia , these are issues that affect all of us .\n",
            "2022-03-20 18:53:17 | INFO | hw5.seq2seq | validation loss:\t2.5360\n",
            "2022-03-20 18:53:17 | INFO | hw5.seq2seq | BLEU = 21.68 57.7/29.4/17.0/10.1 (BP = 0.933 ratio = 0.936 hyp_len = 72083 ref_len = 77050)\n",
            "2022-03-20 18:53:17 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/zhToen/checkpoint30.pt\n",
            "2022-03-20 18:53:18 | INFO | hw5.seq2seq | end of epoch 30\n"
          ]
        }
      ],
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjRwllxPjtf"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "N70Gc6smPi1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(inputs=['./checkpoints/zhToen'], output='./checkpoints/zhToen/avg_last_5_checkpoint.pt', num_epoch_checkpoints=5, num_update_checkpoints=None, checkpoint_upper_bound=None)\n",
            "averaging checkpoints:  ['./checkpoints/zhToen/checkpoint30.pt', './checkpoints/zhToen/checkpoint29.pt', './checkpoints/zhToen/checkpoint28.pt', './checkpoints/zhToen/checkpoint27.pt', './checkpoints/zhToen/checkpoint26.pt']\n",
            "Finished writing averaged checkpoint to ./checkpoints/zhToen/avg_last_5_checkpoint.pt\n"
          ]
        }
      ],
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "\n",
        "checkdir=config.savedir\n",
        "!python ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGMiun8PnZy"
      },
      "source": [
        "## Confirm model weights used to generate submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tvRdivVUPnsU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 20:23:58 | INFO | hw5.seq2seq | loaded checkpoint checkpoints/zhToen/avg_last_5_checkpoint.pt: step=unknown loss=2.536004066467285 bleu=21.679366760451536\n",
            "2022-03-20 20:23:58 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 20:24:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2022-03-20 20:24:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-03-20 20:24:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-03-20 20:24:19 | INFO | hw5.seq2seq | example source: 我覺得沒人在乎我 , 鋃鐺入獄讓我滿腹怨恨 。\n",
            "2022-03-20 20:24:19 | INFO | hw5.seq2seq | example hypothesis: i think nobody cares about me , bonding in jail makes me complain .\n",
            "2022-03-20 20:24:19 | INFO | hw5.seq2seq | example reference: i felt like nobody cared , and i reacted with hostility to my confinement .\n",
            "2022-03-20 20:24:19 | INFO | hw5.seq2seq | validation loss:\t2.5241\n",
            "2022-03-20 20:24:19 | INFO | hw5.seq2seq | BLEU = 21.76 58.5/30.1/17.4/10.4 (BP = 0.916 ratio = 0.920 hyp_len = 70860 ref_len = 77050)\n"
          ]
        }
      ],
      "source": [
        "# checkpoint_last.pt : latest epoch\n",
        "# checkpoint_best.pt : highest validation bleu\n",
        "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
        "\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioAIflXpPsxt"
      },
      "source": [
        "## Generate Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oYMxA8FlPtIq"
      },
      "outputs": [],
      "source": [
        "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            \n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "            \n",
        "    # sort based on the order before preprocess\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    \n",
        "    with open(outfile, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Le4RFWXxjmm0"
      },
      "outputs": [],
      "source": [
        "# generate_prediction(model, task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "wvenyi6BPwnD"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "No active exception to reraise",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/tracy/Projects/ML2022/hw5/HW05(try backtrans).ipynb Cell 80'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c61625043227d/home/tracy/Projects/ML2022/hw5/HW05%28try%20backtrans%29.ipynb#ch0000079vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
          ]
        }
      ],
      "source": [
        "raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0cJE-wPzaU"
      },
      "source": [
        "# Back-translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7uPJ2CP0sm"
      },
      "source": [
        "## Train a backward translation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGHjg2ZP3sV"
      },
      "source": [
        "1. Switch the source_lang and target_lang in **config** \n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-back\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waTGz29UP6WI"
      },
      "source": [
        "## Generate synthetic data with backward model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIeTsPexP8FL"
      },
      "source": [
        "### Download monolingual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "396saD9-QBPY"
      },
      "outputs": [],
      "source": [
        "data_dir = './DATA/rawdata'\n",
        "mono_dataset_name = 'mono'\n",
        "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
        "# mono_prefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# urls = (\n",
        "#     \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted_zh_corpus.deduped.gz\"\n",
        "# )\n",
        "# file_names = (\n",
        "#     'ted_zh_corpus.deduped.gz',\n",
        "# )\n",
        "\n",
        "# for u, f in zip(urls, file_names):\n",
        "#     path = mono_prefix/f\n",
        "#     if not path.exists():\n",
        "#         !wget {u} -O {path}\n",
        "#     else:\n",
        "#         print(f'{f} is exist, skip downloading')\n",
        "#     if path.suffix == \".tgz\":\n",
        "#         !tar -xvf {path} -C {prefix}\n",
        "#     elif path.suffix == \".zip\":\n",
        "#         !unzip -o {path} -d {prefix}\n",
        "#     elif path.suffix == \".gz\":\n",
        "#         !gzip -fkd {path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOVQRHzGQU4-"
      },
      "source": [
        "### TODO: clean corpus\n",
        "\n",
        "1. remove sentences that are too long or too short\n",
        "2. unify punctuation\n",
        "\n",
        "hint: you can use clean_s() defined above to do this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIYmxfUOQSov"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./DATA/rawdata/mono/ted_zh_corpus.deduped.clean exists. skipping clean.\n"
          ]
        }
      ],
      "source": [
        "mono_path = \"./DATA/rawdata/mono/ted_zh_corpus.deduped\"\n",
        "mono_clean = \"./DATA/rawdata/mono/ted_zh_corpus.deduped.clean\"\n",
        "\n",
        "\n",
        "if Path(f'{mono_clean}').exists():\n",
        "    print(f'{mono_clean} exists. skipping clean.')\n",
        "else:\n",
        "    max_len, min_len = 1000, 1\n",
        "    with open(mono_path, 'r') as mono_o:\n",
        "        with open(mono_clean, 'w') as mono_c:\n",
        "            for s in mono_o:\n",
        "                s = s.strip()\n",
        "                s = clean_s(s, 'zh')\n",
        "                s_len = len_s(s,'zh')\n",
        "                if s_len < min_len or s_len > max_len:\n",
        "                    continue\n",
        "                print(s, file=mono_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jegH0bvMQVmR"
      },
      "source": [
        "### TODO: Subword Units\n",
        "\n",
        "Use the spm model of the backward model to tokenize the data into subword units\n",
        "\n",
        "hint: spm model is located at DATA/raw-data/\\[dataset\\]/spm\\[vocab_num\\].model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqgR4uUMQZGY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/mono/spm8000.model exists. skipping spm_train.\n",
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/mono/mono_train.zh exists. skipping spm_encode.\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (mono_prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{mono_prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=mono_clean,\n",
        "        model_prefix=mono_prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' 也可\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )\n",
        "spm_model_mono = spm.SentencePieceProcessor(model_file=str(mono_prefix/f'spm{vocab_size}.model'))\n",
        "\n",
        "\n",
        "out_path = mono_prefix/f'mono_train.zh'\n",
        "if out_path.exists():\n",
        "    print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "else:\n",
        "    with open(f\"{out_path}\", 'w') as out_f:\n",
        "        with open(mono_clean, 'r') as in_f:\n",
        "            for line in in_f:\n",
        "                line = line.strip()\n",
        "                tok = spm_model_mono.encode(line, out_type=str)\n",
        "                print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/tracy/Projects/ML2022/hw5/DATA/rawdata/mono/mono_train.en exists. skipping generation.\n"
          ]
        }
      ],
      "source": [
        "# Generate fake mono/train.en\n",
        "out_path_en = mono_prefix/f'mono_train.en'\n",
        "if out_path_en.exists():\n",
        "    print(f\"{out_path_en} exists. skipping generation.\")\n",
        "else:\n",
        "    with open(f\"{out_path_en}\", 'w') as out_f:\n",
        "        for _ in range(781713): # 781713 is the length of mono_clean\n",
        "            line = '▁。\\n'\n",
        "            out_f.write(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a65glBVXQZiE"
      },
      "source": [
        "### Binarize\n",
        "\n",
        "use fairseq to binarize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b803qA5aQaEu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA/data-bin/mono exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', mono_dataset_name)\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = str(mono_prefix/\"mono_train\") # whatever filepath you get after applying subword tokenization\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smA0JraEQdxz"
      },
      "source": [
        "### TODO: Generate synthetic data with backward model\n",
        "\n",
        "Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "\n",
        "ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "\n",
        "then you can use 'generate_prediction(model, task, split=\"split_name\")' to generate translation prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvaOVHeoQfkB"
      },
      "outputs": [],
      "source": [
        "# Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "# ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "# !cp ./DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
        "# !cp ./DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
        "# !cp ./DATA/data-bin/mono/train.zh-en.en.bin ./DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
        "# !cp ./DATA/data-bin/mono/train.zh-en.en.idx ./DATA/data-bin/ted2020/mono.zh-en.en.idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fFEkxPu-Qhlc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-20 20:25:16 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020/mono.zh-en.zh\n",
            "2022-03-20 20:25:16 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020/mono.zh-en.en\n",
            "2022-03-20 20:25:16 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 mono zh-en 781713 examples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prediction: 100%|██████████| 1572/1572 [35:28<00:00,  1.35s/it]\n"
          ]
        }
      ],
      "source": [
        "# hint: do prediction on split='mono' to create prediction_file\n",
        "# generate_prediction(model,task,split=\"mono\" ,outfile=\"./mono_prediction_en.txt\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn4XeawpQjLk"
      },
      "source": [
        "### TODO: Create new dataset\n",
        "\n",
        "1. Combine the prediction data with monolingual data\n",
        "2. Use the original spm model to tokenize data into Subword Units\n",
        "3. Binarize data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3R35JTaTQjkm"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'mono_prefix' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/tracy/Projects/ML2022/hw5/HW05(try backtrans).ipynb Cell 98'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c61625043227d/home/tracy/Projects/ML2022/hw5/HW05%28try%20backtrans%29.ipynb#ch0000097vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msentencepiece\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspm\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c61625043227d/home/tracy/Projects/ML2022/hw5/HW05%28try%20backtrans%29.ipynb#ch0000097vscode-remote?line=1'>2</a>\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39m8000\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c61625043227d/home/tracy/Projects/ML2022/hw5/HW05%28try%20backtrans%29.ipynb#ch0000097vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m (mono_prefix\u001b[39m/\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mspm\u001b[39m\u001b[39m{\u001b[39;00mvocab_size\u001b[39m}\u001b[39;00m\u001b[39m.model\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mexists():\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c61625043227d/home/tracy/Projects/ML2022/hw5/HW05%28try%20backtrans%29.ipynb#ch0000097vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmono_prefix\u001b[39m}\u001b[39;00m\u001b[39m/spm\u001b[39m\u001b[39m{\u001b[39;00mvocab_size\u001b[39m}\u001b[39;00m\u001b[39m.model exists. skipping spm_train.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c61625043227d/home/tracy/Projects/ML2022/hw5/HW05%28try%20backtrans%29.ipynb#ch0000097vscode-remote?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mono_prefix' is not defined"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (mono_prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{mono_prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=mono_clean,\n",
        "        model_prefix=mono_prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' 也可\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )\n",
        "spm_model_mono = spm.SentencePieceProcessor(model_file=str(mono_prefix/f'spm{vocab_size}.model'))\n",
        "# Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.\n",
        "# \n",
        "# hint: tokenize prediction_file with the spm model\n",
        "# spm_model.encode(line, out_type=str)\n",
        "# output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
        "pred_file = \"./mono_prediction_en.txt\"\n",
        "out_file = \"./DATA/rawdata/mono/mono_train.en\"\n",
        "with open(out_file, 'w') as out_f:\n",
        "    with open(pred_file, 'r') as in_f:\n",
        "        for line in in_f:\n",
        "            line = line.strip()\n",
        "            tok = spm_model_mono.encode(line, out_type=str)\n",
        "            print(' '.join(tok), file=out_f)\n",
        "\n",
        "# hint: use fairseq to binarize these two files again\n",
        "binpath = Path('./DATA/data-bin/synthetic')\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = \"./DATA/rawdata/mono/mono_train\" # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "MSkse1tyQnsR"
      },
      "outputs": [],
      "source": [
        "# create a new dataset from all the files prepared above\n",
        "!cp -r ./DATA/data-bin/ted2020/ ./DATA/data-bin/ted2020_with_mono/\n",
        "\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVdxVGO3QrSs"
      },
      "source": [
        "Created new dataset \"ted2020_with_mono\"\n",
        "\n",
        "1. Change the datadir in **config** (\"./DATA/data-bin/ted2020_with_mono\")\n",
        "2. Switch back the source_lang and target_lang in **config** (\"en\", \"zh\")\n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-bt\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CZU2beUQtl3"
      },
      "source": [
        "1. <a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).\n",
        "2. <a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).\n",
        "3. <a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., & Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n",
        "4. <a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).\n",
        "5. <a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., & Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).\n",
        "6. <a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., & Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).\n",
        "7. <a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).\n",
        "8. https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n",
        "9. https://ithelp.ithome.com.tw/articles/10233122\n",
        "10. https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "11. https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW05/HW05.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train en to zh model with ted with mono dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Rrfm6iLJQ0tS"
      },
      "outputs": [],
      "source": [
        "# config a en to zh model\n",
        "\n",
        "config = Namespace(\n",
        "    datadir = \"./DATA/data-bin/ted2020_with_mono\",\n",
        "    savedir = \"./checkpoints/transformer-bt2\",\n",
        "    source_lang = \"en\",\n",
        "    target_lang = \"zh\",\n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=3,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=8192//2,\n",
        "    accum_steps=8,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=30,\n",
        "    start_epoch=18,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10, \n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-22 23:53:51 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
            "2022-03-22 23:53:51 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n",
            "2022-03-22 23:53:51 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2022-03-22 23:53:51 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020_with_mono/train.en-zh.en\n",
            "2022-03-22 23:53:51 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020_with_mono/train.en-zh.zh\n",
            "2022-03-22 23:53:51 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono train en-zh 390041 examples\n",
            "2022-03-22 23:53:51 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en\n",
            "2022-03-22 23:53:51 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh\n",
            "2022-03-22 23:53:51 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono train1 en-zh 781713 examples\n",
            "2022-03-22 23:53:51 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020_with_mono/valid.en-zh.en\n",
            "2022-03-22 23:53:51 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020_with_mono/valid.en-zh.zh\n",
            "2022-03-22 23:53:51 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono valid en-zh 3939 examples\n"
          ]
        }
      ],
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)\n",
        "\n",
        "\n",
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "new_task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(new_task_cfg)\n",
        "\n",
        "\n",
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_arch_args = Namespace(\n",
        "    encoder_embed_dim=1024,\n",
        "    encoder_ffn_embed_dim=4096,\n",
        "    encoder_layers=6,\n",
        "    decoder_embed_dim=1024,\n",
        "    decoder_ffn_embed_dim=4096,\n",
        "    decoder_layers=6,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.2,\n",
        ")\n",
        "\n",
        "# HINT: these patches on parameters for Transformer\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=8\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=8\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(new_arch_args)\n",
        "\n",
        "add_transformer_args(new_arch_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(new_arch_args))\n",
        "\n",
        "model = build_model(new_arch_args, task).to(device=device)\n",
        "# logger.info(model)\n",
        "\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")\n",
        "\n",
        "optimizer = NoamOpt(\n",
        "    model_size=new_arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-22 23:53:57 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2022-03-22 23:53:57 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2022-03-22 23:53:57 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2022-03-22 23:53:57 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-03-22 23:53:57 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2022-03-22 23:53:57 | INFO | hw5.seq2seq | num. model params: 192,745,472 (num. trained: 192,745,472)\n",
            "2022-03-22 23:53:57 | INFO | hw5.seq2seq | max tokens per batch = 4096, accumulate steps = 8\n",
            "2022-03-22 23:53:58 | WARNING | fairseq.tasks.fairseq_task | 18 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[561674, 686752, 667498, 594020, 637897, 671087, 756446, 450067, 326674, 657259]\n",
            "2022-03-22 23:53:58 | INFO | hw5.seq2seq | loaded checkpoint checkpoints/transformer-bt2/checkpoint_last.pt: step=28081 loss=2.947722911834717 bleu=28.487116038329624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 00:20:41 | INFO | hw5.seq2seq | training loss: 2.3227\n",
            "2022-03-23 00:20:41 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "validation:   0%|          | 0/40 [00:00<?, ?it/s, valid_loss=2.75]/home/tracy/miniconda3/envs/torch/lib/python3.9/site-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/home/tracy/miniconda3/envs/torch/lib/python3.9/site-packages/fairseq/sequence_generator.py:657: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n",
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 00:21:12 | INFO | hw5.seq2seq | example source: i take them backwards here to their independence , when they were up here in the beginning of the 1960s .\n",
            "2022-03-23 00:21:12 | INFO | hw5.seq2seq | example hypothesis: 我把它們倒退到了這裡 , 直到1960年代初期 , 當它們在這上面的時候 。\n",
            "2022-03-23 00:21:12 | INFO | hw5.seq2seq | example reference: 讓我們回到他們獨立的時候那是在1960年代初\n",
            "2022-03-23 00:21:12 | INFO | hw5.seq2seq | validation loss:\t2.9449\n",
            "2022-03-23 00:21:12 | INFO | hw5.seq2seq | BLEU = 28.53 60.2/35.8/22.6/14.9 (BP = 0.977 ratio = 0.977 hyp_len = 109278 ref_len = 111811)\n",
            "2022-03-23 00:21:14 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint18.pt\n",
            "2022-03-23 00:21:17 | INFO | hw5.seq2seq | end of epoch 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 00:48:20 | INFO | hw5.seq2seq | training loss: 2.2954\n",
            "2022-03-23 00:48:20 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 00:48:52 | INFO | hw5.seq2seq | example source: but when i stepped back , i felt myself at the cold , hard center of a perfect storm .\n",
            "2022-03-23 00:48:52 | INFO | hw5.seq2seq | example hypothesis: 但當我向後退了一步 , 我覺得自己處在一場完美風暴的寒冷中央 。\n",
            "2022-03-23 00:48:52 | INFO | hw5.seq2seq | example reference: 但當我退一步看我發現我自己正處在冷冰冰鐵一般硬的完美風暴的中心\n",
            "2022-03-23 00:48:52 | INFO | hw5.seq2seq | validation loss:\t2.9499\n",
            "2022-03-23 00:48:52 | INFO | hw5.seq2seq | BLEU = 28.81 60.3/36.0/22.8/15.2 (BP = 0.979 ratio = 0.979 hyp_len = 109473 ref_len = 111811)\n",
            "2022-03-23 00:48:54 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint19.pt\n",
            "2022-03-23 00:48:57 | INFO | hw5.seq2seq | end of epoch 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 01:16:04 | INFO | hw5.seq2seq | training loss: 2.2716\n",
            "2022-03-23 01:16:04 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 01:16:34 | INFO | hw5.seq2seq | example source: we have major challenges , you can imagine , to finish this job , but as you've also seen , it's doable , it has great secondary benefits , and polio eradication is a great buy .\n",
            "2022-03-23 01:16:34 | INFO | hw5.seq2seq | example hypothesis: 我們有重大的挑戰 , 你可以想像 , 要完成這項工作 , 但如你也看到的 , 它是可行的 , 它有中間的好處 , 根除小兒麻痺症是很好的購買 。\n",
            "2022-03-23 01:16:34 | INFO | hw5.seq2seq | example reference: 你們可以想像 , 要完成這項任務是種巨大的挑戰但各位也看到了這是可行的它的次要效益是很大的根絕小兒麻痺症是值得投資的\n",
            "2022-03-23 01:16:34 | INFO | hw5.seq2seq | validation loss:\t2.9509\n",
            "2022-03-23 01:16:34 | INFO | hw5.seq2seq | BLEU = 28.48 60.6/36.1/22.9/15.2 (BP = 0.964 ratio = 0.965 hyp_len = 107842 ref_len = 111811)\n",
            "2022-03-23 01:16:37 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint20.pt\n",
            "2022-03-23 01:16:37 | INFO | hw5.seq2seq | end of epoch 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 01:43:48 | INFO | hw5.seq2seq | training loss: 2.2497\n",
            "2022-03-23 01:43:48 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 01:44:20 | INFO | hw5.seq2seq | example source: the mayor defined the city's new normal as one of permanent drought .\n",
            "2022-03-23 01:44:20 | INFO | hw5.seq2seq | example hypothesis: 市長把該城市的新常態定義為永久性的乾旱 。\n",
            "2022-03-23 01:44:20 | INFO | hw5.seq2seq | example reference: 市長把該市的新常態定義為永久的乾旱 。\n",
            "2022-03-23 01:44:20 | INFO | hw5.seq2seq | validation loss:\t2.9484\n",
            "2022-03-23 01:44:20 | INFO | hw5.seq2seq | BLEU = 28.86 60.4/36.1/22.9/15.2 (BP = 0.978 ratio = 0.978 hyp_len = 109372 ref_len = 111811)\n",
            "2022-03-23 01:44:23 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint21.pt\n",
            "2022-03-23 01:44:25 | INFO | hw5.seq2seq | end of epoch 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 02:11:38 | INFO | hw5.seq2seq | training loss: 2.2296\n",
            "2022-03-23 02:11:38 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 02:12:10 | INFO | hw5.seq2seq | example source: so with friends and colleagues , he started a community radio station , he rented a video camera , and he's now making films .\n",
            "2022-03-23 02:12:10 | INFO | hw5.seq2seq | example hypothesis: 所以和朋友及同事一起 , 他創立了一個社區電台 , 他租了一台攝影機 , 他現在製作電影 。\n",
            "2022-03-23 02:12:10 | INFO | hw5.seq2seq | example reference: 所以他和朋友同事們一起 , 開辦了一個社區無線電台 , 他租了一臺攝影機 , 然後現在在拍電影 。\n",
            "2022-03-23 02:12:10 | INFO | hw5.seq2seq | validation loss:\t2.9527\n",
            "2022-03-23 02:12:10 | INFO | hw5.seq2seq | BLEU = 28.71 60.7/36.3/23.0/15.4 (BP = 0.966 ratio = 0.966 hyp_len = 108023 ref_len = 111811)\n",
            "2022-03-23 02:12:12 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint22.pt\n",
            "2022-03-23 02:12:12 | INFO | hw5.seq2seq | end of epoch 22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 02:39:27 | INFO | hw5.seq2seq | training loss: 2.2092\n",
            "2022-03-23 02:39:27 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 02:39:58 | INFO | hw5.seq2seq | example source: i'm kidding .\n",
            "2022-03-23 02:39:58 | INFO | hw5.seq2seq | example hypothesis: 我開玩笑的 。\n",
            "2022-03-23 02:39:58 | INFO | hw5.seq2seq | example reference: 我是在開玩笑 。\n",
            "2022-03-23 02:39:58 | INFO | hw5.seq2seq | validation loss:\t2.9587\n",
            "2022-03-23 02:39:58 | INFO | hw5.seq2seq | BLEU = 28.69 60.7/36.2/23.0/15.3 (BP = 0.968 ratio = 0.968 hyp_len = 108261 ref_len = 111811)\n",
            "2022-03-23 02:40:01 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint23.pt\n",
            "2022-03-23 02:40:01 | INFO | hw5.seq2seq | end of epoch 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 03:07:16 | INFO | hw5.seq2seq | training loss: 2.1914\n",
            "2022-03-23 03:07:16 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 03:07:47 | INFO | hw5.seq2seq | example source: and in fact , right now in many states , you can go out and you can buy bioluminescent pets .\n",
            "2022-03-23 03:07:47 | INFO | hw5.seq2seq | example hypothesis: 事實上 , 在許多州你可以出去購買生物發光的寵物\n",
            "2022-03-23 03:07:47 | INFO | hw5.seq2seq | example reference: 而事實上 , 目前在許多州 , 你可以去寵物店買發光寵物\n",
            "2022-03-23 03:07:47 | INFO | hw5.seq2seq | validation loss:\t2.9652\n",
            "2022-03-23 03:07:47 | INFO | hw5.seq2seq | BLEU = 28.66 60.3/35.9/22.7/15.2 (BP = 0.975 ratio = 0.975 hyp_len = 109037 ref_len = 111811)\n",
            "2022-03-23 03:07:50 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint24.pt\n",
            "2022-03-23 03:07:50 | INFO | hw5.seq2seq | end of epoch 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 03:35:05 | INFO | hw5.seq2seq | training loss: 2.1742\n",
            "2022-03-23 03:35:05 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 03:35:37 | INFO | hw5.seq2seq | example source: we actually take a very small piece of the bladder from the patient less than half the size of a postage stamp .\n",
            "2022-03-23 03:35:37 | INFO | hw5.seq2seq | example hypothesis: 我們從病人身上取下一塊非常小的膀胱 , 比郵票的一半還小 。\n",
            "2022-03-23 03:35:37 | INFO | hw5.seq2seq | example reference: 我們從病人身上取下一片非常小的膀胱組織--比一張郵票的一半還小 。\n",
            "2022-03-23 03:35:37 | INFO | hw5.seq2seq | validation loss:\t2.9693\n",
            "2022-03-23 03:35:37 | INFO | hw5.seq2seq | BLEU = 28.75 60.6/36.2/23.0/15.4 (BP = 0.969 ratio = 0.969 hyp_len = 108374 ref_len = 111811)\n",
            "2022-03-23 03:35:40 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint25.pt\n",
            "2022-03-23 03:35:40 | INFO | hw5.seq2seq | end of epoch 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 04:02:55 | INFO | hw5.seq2seq | training loss: 2.1575\n",
            "2022-03-23 04:02:55 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 04:03:26 | INFO | hw5.seq2seq | example source: institutions hate being told they're obstacles .\n",
            "2022-03-23 04:03:26 | INFO | hw5.seq2seq | example hypothesis: 機構討厭被告知他們是阻礙 。\n",
            "2022-03-23 04:03:26 | INFO | hw5.seq2seq | example reference: 機構討厭被人家稱為阻礙 。\n",
            "2022-03-23 04:03:26 | INFO | hw5.seq2seq | validation loss:\t2.9700\n",
            "2022-03-23 04:03:26 | INFO | hw5.seq2seq | BLEU = 28.77 60.4/36.1/22.9/15.3 (BP = 0.973 ratio = 0.974 hyp_len = 108857 ref_len = 111811)\n",
            "2022-03-23 04:03:29 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint26.pt\n",
            "2022-03-23 04:03:29 | INFO | hw5.seq2seq | end of epoch 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 04:30:48 | INFO | hw5.seq2seq | training loss: 2.1416\n",
            "2022-03-23 04:30:48 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 04:31:22 | INFO | hw5.seq2seq | example source: he was in sudan when the civil war broke out there .\n",
            "2022-03-23 04:31:22 | INFO | hw5.seq2seq | example hypothesis: 當內戰爆發時他在蘇丹\n",
            "2022-03-23 04:31:22 | INFO | hw5.seq2seq | example reference: 蘇丹的內戰爆發時他也在\n",
            "2022-03-23 04:31:22 | INFO | hw5.seq2seq | validation loss:\t2.9806\n",
            "2022-03-23 04:31:22 | INFO | hw5.seq2seq | BLEU = 28.59 60.8/36.4/23.0/15.3 (BP = 0.962 ratio = 0.962 hyp_len = 107614 ref_len = 111811)\n",
            "2022-03-23 04:31:24 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint27.pt\n",
            "2022-03-23 04:31:24 | INFO | hw5.seq2seq | end of epoch 27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 04:58:41 | INFO | hw5.seq2seq | training loss: 2.1268\n",
            "2022-03-23 04:58:41 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 04:59:12 | INFO | hw5.seq2seq | example source: have sex .\n",
            "2022-03-23 04:59:12 | INFO | hw5.seq2seq | example hypothesis: 做愛吧 。\n",
            "2022-03-23 04:59:12 | INFO | hw5.seq2seq | example reference: 有性生活 。\n",
            "2022-03-23 04:59:12 | INFO | hw5.seq2seq | validation loss:\t2.9756\n",
            "2022-03-23 04:59:12 | INFO | hw5.seq2seq | BLEU = 28.76 60.5/36.1/23.0/15.4 (BP = 0.970 ratio = 0.971 hyp_len = 108549 ref_len = 111811)\n",
            "2022-03-23 04:59:15 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint28.pt\n",
            "2022-03-23 04:59:15 | INFO | hw5.seq2seq | end of epoch 28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 05:26:29 | INFO | hw5.seq2seq | training loss: 2.1122\n",
            "2022-03-23 05:26:29 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 05:27:00 | INFO | hw5.seq2seq | example source: but i'll tell you some of that today .\n",
            "2022-03-23 05:27:00 | INFO | hw5.seq2seq | example hypothesis: 但今天我會告訴各位其中一些事 。\n",
            "2022-03-23 05:27:00 | INFO | hw5.seq2seq | example reference: 但我今天會告訴你們其中的一部分 。\n",
            "2022-03-23 05:27:00 | INFO | hw5.seq2seq | validation loss:\t2.9792\n",
            "2022-03-23 05:27:00 | INFO | hw5.seq2seq | BLEU = 28.46 60.7/36.3/23.0/15.4 (BP = 0.957 ratio = 0.958 hyp_len = 107074 ref_len = 111811)\n",
            "2022-03-23 05:27:03 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint29.pt\n",
            "2022-03-23 05:27:03 | INFO | hw5.seq2seq | end of epoch 29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 05:54:20 | INFO | hw5.seq2seq | training loss: 2.0987\n",
            "2022-03-23 05:54:20 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 05:54:51 | INFO | hw5.seq2seq | example source: all the processing to do that was actually done on the device itself .\n",
            "2022-03-23 05:54:51 | INFO | hw5.seq2seq | example hypothesis: 所有處理過程都是在這個裝置上進行 。\n",
            "2022-03-23 05:54:51 | INFO | hw5.seq2seq | example reference: 所有需要處理的步驟實際上都在手機上完成\n",
            "2022-03-23 05:54:51 | INFO | hw5.seq2seq | validation loss:\t2.9853\n",
            "2022-03-23 05:54:51 | INFO | hw5.seq2seq | BLEU = 28.69 60.5/36.1/22.8/15.3 (BP = 0.972 ratio = 0.972 hyp_len = 108681 ref_len = 111811)\n",
            "2022-03-23 05:54:54 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/tracy/Projects/ML2022/hw5/checkpoints/transformer-bt2/checkpoint30.pt\n",
            "2022-03-23 05:54:54 | INFO | hw5.seq2seq | end of epoch 30\n"
          ]
        }
      ],
      "source": [
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")\n",
        "\n",
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)\n",
        "\n",
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(inputs=['checkpoints/best_enesemble'], output='checkpoints/best_enesemble/avg_best_2_checkpoint.pt', num_epoch_checkpoints=2, num_update_checkpoints=None, checkpoint_upper_bound=None)\n",
            "averaging checkpoints:  ['checkpoints/best_enesemble/checkpoint2.pt', 'checkpoints/best_enesemble/checkpoint1.pt']\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/tracy/Projects/ML2022/hw5/./fairseq/scripts/average_checkpoints.py\", line 158, in <module>\n",
            "    main()\n",
            "  File \"/home/tracy/Projects/ML2022/hw5/./fairseq/scripts/average_checkpoints.py\", line 151, in main\n",
            "    new_state = average_checkpoints(args.inputs)\n",
            "  File \"/home/tracy/Projects/ML2022/hw5/./fairseq/scripts/average_checkpoints.py\", line 63, in average_checkpoints\n",
            "    params_dict[k] += p\n",
            "RuntimeError: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        }
      ],
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "\n",
        "checkdir=config.savedir\n",
        "!python ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 10:23:05 | INFO | hw5.seq2seq | loaded checkpoint checkpoints/transformer-bt2/checkpoint_best.pt: step=unknown loss=2.9484353065490723 bleu=28.8571603790927\n"
          ]
        }
      ],
      "source": [
        "model = build_model(new_arch_args, task).to(device=device)\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-03-23 10:23:16 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020_with_mono/test.en-zh.en\n",
            "2022-03-23 10:23:16 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020_with_mono/test.en-zh.zh\n",
            "2022-03-23 10:23:16 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono test en-zh 4000 examples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "prediction:   0%|          | 0/32 [00:00<?, ?it/s]/home/tracy/miniconda3/envs/torch/lib/python3.9/site-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/home/tracy/miniconda3/envs/torch/lib/python3.9/site-packages/fairseq/sequence_generator.py:657: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n",
            "prediction: 100%|██████████| 32/32 [00:24<00:00,  1.32it/s]\n"
          ]
        }
      ],
      "source": [
        "sequence_generator = task.build_generator([model], config)\n",
        "generate_prediction(model,task,outfile=\"./combined_prediction_zh.txt\" )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nKb4u67-sT_Z",
        "n1rwQysTsdJq",
        "59si_C0Wsms7",
        "oOpG4EBRLwe_",
        "6ZlE_1JnMv56",
        "UDAPmxjRNEEL",
        "ce5n4eS7NQNy",
        "rUB9f1WCNgMH",
        "VFJlkOMONsc6",
        "Gt1lX3DRO_yU",
        "BAGMiun8PnZy",
        "JOVQRHzGQU4-",
        "jegH0bvMQVmR",
        "a65glBVXQZiE",
        "smA0JraEQdxz",
        "Jn4XeawpQjLk"
      ],
      "name": "HW05.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "842cd891a2846f4bdf58fcb50c5800efd1dfa8c7ba084c9fc1192f566055c2af"
    },
    "kernelspec": {
      "display_name": "torch11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
